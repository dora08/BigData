cat /home/cloudera/employee.txt
hdfs dfs - ls /
sudo su
sudo -u hdfs hadoop fs -mkdir /inputdirectory
#if anyone get error then type
hdfs dfsadmin safemode leave
#if not then type this
sudo -u hdfs hadoop fs -mkdir /inputdirectory
sudo -u hdfs hadoop fs -chmod -R 777 /inputdirectory
sudo -u hdfs hadoop fs -put /home/cloudera/employee.txt /inputdirectory
hdfs dfs -ls /
hdfs dfs -ls /inputdirectory
hadoop fs -cat /inputdirectory
hadoop fs -cat /inputdirectory/employee.txt


hive
show databases;
create database organization;
show databases;
use organization;
show tables;

CREATE TABLE employee (id INT, name STRING, city STRING, department STRING, salary INT, domain STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '~' STORED AS TEXTFILE;




show tables;
select * from employee;
show tables;
load data inpath ‘/inputdirectory/employee.txt’ overwrite into table employee;
select * from employee;


(The data that we entered as input in the employee.txt file is loaded into the
databases i.e. we created in hive with proper data types and tilt sign ‘~’. So,
we got that data in proper table format) 
